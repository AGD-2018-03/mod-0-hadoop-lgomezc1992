{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuerde no agregar o quitar celdas en este notebook, ni modificar su tipo. Si lo hace, el sistema automaticamente lo calificará con cero punto cero (0.0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenga las letras asociadas a cada clave para el siguiente archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "0 \t C,F,A,B,D,I,H\n",
    "1 \t A,H,C,I,F,D,B\n",
    "2 \t B,H,I,F,G\n",
    "3 \t C,B,D\n",
    "4 \t D,C,I,G,H\n",
    "5 \t B,D,C,H,A\n",
    "6 \t H,D,C,B,G,F,D\n",
    "7 \t F,A,G,C,B,D,H,I\n",
    "8 \t F,A,I,B,D\n",
    "9 \t F,A,B,D,C,D,G,I\n",
    "10 \t D,B,A,C,H\n",
    "11 \t G,D,B,H,I,C,F\n",
    "12 \t D,D,C,F,B,A,H,G\n",
    "13 \t F,A,D\n",
    "14 \t D,A,C,G\n",
    "15 \t H,A,F,D,B,C,G,I\n",
    "16 \t A,I,B,D\n",
    "17 \t C,B,G,A,D,H,F\n",
    "18 \t I,B,A,H,D,F\n",
    "19 \t B,D,F,D,I\n",
    "20 \t C,B,H,F,I,G,D,D\n",
    "21 \t F,A,B,C,G,D\n",
    "22 \t I,G,F,C,H,B\n",
    "23 \t H,F,C,B,D,D,A\n",
    "24 \t F,D,G,A,H\n",
    "25 \t G,H,B,C,A,F,I\n",
    "26 \t G,F,B,A,H,D,D,I\n",
    "27 \t B,A,H,I,D,G,F\n",
    "28 \t A,H,D,F,C\n",
    "29 \t C,D,A,F,G,B,H,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    lineas = sys.stdin.readlines()\n",
    "    \n",
    "    for line in lineas:\n",
    "        key=line.split('\\t')[0]\n",
    "        data=line.split('\\t')[-1].replace('\\n','').replace(',','').replace(' ','')\n",
    "        for word in data:\n",
    "            sys.stdout.write(\"{}\\t{}\\n\".format(word,str(key).zfill(4)))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__': \n",
    "    curkey = None\n",
    "    data = None\n",
    "    lineas = sys.stdin.readlines()\n",
    "    for line in lineas:\n",
    "        key, val = line.split(\"\\t\")\n",
    "        val=int(val.replace('\\n',''))\n",
    "               \n",
    "        if key == curkey:\n",
    "            if data==None:\n",
    "                data=str(val)\n",
    "            else:\n",
    "                data=str(data)+','+str(val)\n",
    "        else:\n",
    "            if curkey is not None:\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey,data)) \n",
    "            curkey = key\n",
    "            data=val\n",
    "\n",
    "    if key == curkey:\n",
    "        sys.stdout.write(\"{}\\t{}\\n\".format(curkey, data)) \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t1,18,26,21,10,7,0,12,25,8,27,13,14,15,24,29,16,28,23,9,17,5\n",
      "B\t22,19,20,21,1,29,2,3,5,6,7,27,8,9,26,11,12,0,25,15,16,17,23,18,10\n",
      "C\t0,29,28,25,23,22,21,20,17,15,14,12,11,10,9,7,6,5,4,3,1\n",
      "D\t19,19,18,17,23,23,1,16,24,15,14,13,29,12,12,11,10,9,9,26,26,8,7,27,6,0,6,5,4,3,28,21,20,20,29\n",
      "F\t18,25,6,20,9,0,1,23,11,8,24,27,28,17,21,19,13,29,26,12,22,7,15,2\n",
      "G\t22,6,27,26,11,25,2,9,20,24,14,21,15,12,29,17,7,4\n",
      "H\t25,12,1,11,10,26,0,2,20,27,7,22,6,18,23,17,5,28,4,24,15,29\n",
      "I\t7,16,25,15,8,26,2,9,11,4,22,18,20,0,27,19,1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 17:17:40,647 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-11-05 17:17:41,456 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-11-05 17:17:41,564 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-11-05 17:17:41,564 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-11-05 17:17:41,593 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-05 17:17:42,211 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2018-11-05 17:17:42,305 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2018-11-05 17:17:42,624 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1178982250_0001\n",
      "2018-11-05 17:17:42,626 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-11-05 17:17:42,869 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-11-05 17:17:42,872 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-11-05 17:17:42,872 INFO mapreduce.Job: Running job: job_local1178982250_0001\n",
      "2018-11-05 17:17:42,875 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2018-11-05 17:17:42,883 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-05 17:17:42,883 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-05 17:17:43,008 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-11-05 17:17:43,014 INFO mapred.LocalJobRunner: Starting task: attempt_local1178982250_0001_m_000000_0\n",
      "2018-11-05 17:17:43,051 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-05 17:17:43,051 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-05 17:17:43,067 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-11-05 17:17:43,067 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-11-05 17:17:43,081 INFO mapred.MapTask: Processing split: file:/Users/eduardogomez/Talleres_Analitica_Grandes_Datos/input.txt:0+507\n",
      "2018-11-05 17:17:43,102 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-11-05 17:17:43,222 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-11-05 17:17:43,222 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-11-05 17:17:43,222 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-11-05 17:17:43,222 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-11-05 17:17:43,222 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-11-05 17:17:43,227 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-11-05 17:17:43,241 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/eduardogomez/Talleres_Analitica_Grandes_Datos/./mapper.py]\n",
      "2018-11-05 17:17:43,251 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2018-11-05 17:17:43,252 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2018-11-05 17:17:43,253 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2018-11-05 17:17:43,253 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2018-11-05 17:17:43,254 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2018-11-05 17:17:43,254 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2018-11-05 17:17:43,255 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2018-11-05 17:17:43,256 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-11-05 17:17:43,256 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2018-11-05 17:17:43,257 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2018-11-05 17:17:43,258 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-11-05 17:17:43,259 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2018-11-05 17:17:43,292 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-05 17:17:43,293 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-05 17:17:43,367 INFO streaming.PipeMapRed: Records R/W=30/1\n",
      "2018-11-05 17:17:43,372 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-11-05 17:17:43,374 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-11-05 17:17:43,378 INFO mapred.LocalJobRunner: \n",
      "2018-11-05 17:17:43,378 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-11-05 17:17:43,378 INFO mapred.MapTask: Spilling map output\n",
      "2018-11-05 17:17:43,378 INFO mapred.MapTask: bufstart = 0; bufend = 1288; bufvoid = 104857600\n",
      "2018-11-05 17:17:43,378 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213664(104854656); length = 733/6553600\n",
      "2018-11-05 17:17:43,423 INFO mapred.MapTask: Finished spill 0\n",
      "2018-11-05 17:17:43,448 INFO mapred.Task: Task:attempt_local1178982250_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-11-05 17:17:43,451 INFO mapred.LocalJobRunner: Records R/W=30/1\n",
      "2018-11-05 17:17:43,451 INFO mapred.Task: Task 'attempt_local1178982250_0001_m_000000_0' done.\n",
      "2018-11-05 17:17:43,466 INFO mapred.Task: Final Counters for attempt_local1178982250_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176993\n",
      "\t\tFILE: Number of bytes written=687414\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=184\n",
      "\t\tMap output bytes=1288\n",
      "\t\tMap output materialized bytes=1662\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=184\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=215482368\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=507\n",
      "2018-11-05 17:17:43,466 INFO mapred.LocalJobRunner: Finishing task: attempt_local1178982250_0001_m_000000_0\n",
      "2018-11-05 17:17:43,468 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-11-05 17:17:43,471 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-11-05 17:17:43,479 INFO mapred.LocalJobRunner: Starting task: attempt_local1178982250_0001_r_000000_0\n",
      "2018-11-05 17:17:43,490 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-11-05 17:17:43,490 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-11-05 17:17:43,491 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-11-05 17:17:43,491 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-11-05 17:17:43,495 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7ed7a62b\n",
      "2018-11-05 17:17:43,497 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-05 17:17:43,524 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=668309888, maxSingleShuffleLimit=167077472, mergeThreshold=441084544, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-11-05 17:17:43,527 INFO reduce.EventFetcher: attempt_local1178982250_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-11-05 17:17:43,562 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1178982250_0001_m_000000_0 decomp: 1658 len: 1662 to MEMORY\n",
      "2018-11-05 17:17:43,572 INFO reduce.InMemoryMapOutput: Read 1658 bytes from map-output for attempt_local1178982250_0001_m_000000_0\n",
      "2018-11-05 17:17:43,574 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1658, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1658\n",
      "2018-11-05 17:17:43,576 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-11-05 17:17:43,577 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-11-05 17:17:43,578 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-11-05 17:17:43,604 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-11-05 17:17:43,605 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1654 bytes\n",
      "2018-11-05 17:17:43,620 INFO reduce.MergeManagerImpl: Merged 1 segments, 1658 bytes to disk to satisfy reduce memory limit\n",
      "2018-11-05 17:17:43,621 INFO reduce.MergeManagerImpl: Merging 1 files, 1662 bytes from disk\n",
      "2018-11-05 17:17:43,622 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-11-05 17:17:43,622 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-11-05 17:17:43,623 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1654 bytes\n",
      "2018-11-05 17:17:43,623 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-11-05 17:17:43,635 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/eduardogomez/Talleres_Analitica_Grandes_Datos/./reducer.py]\n",
      "2018-11-05 17:17:43,638 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-11-05 17:17:43,639 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-11-05 17:17:43,703 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-05 17:17:43,704 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-05 17:17:43,708 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-11-05 17:17:43,741 INFO streaming.PipeMapRed: Records R/W=184/1\n",
      "2018-11-05 17:17:43,747 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-11-05 17:17:43,747 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-11-05 17:17:43,749 INFO mapred.Task: Task:attempt_local1178982250_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-11-05 17:17:43,750 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-11-05 17:17:43,750 INFO mapred.Task: Task attempt_local1178982250_0001_r_000000_0 is allowed to commit now\n",
      "2018-11-05 17:17:43,753 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1178982250_0001_r_000000_0' to file:/Users/eduardogomez/Talleres_Analitica_Grandes_Datos/output\n",
      "2018-11-05 17:17:43,755 INFO mapred.LocalJobRunner: Records R/W=184/1 > reduce\n",
      "2018-11-05 17:17:43,755 INFO mapred.Task: Task 'attempt_local1178982250_0001_r_000000_0' done.\n",
      "2018-11-05 17:17:43,756 INFO mapred.Task: Final Counters for attempt_local1178982250_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180349\n",
      "\t\tFILE: Number of bytes written=689596\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1662\n",
      "\t\tReduce input records=184\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=216006656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=520\n",
      "2018-11-05 17:17:43,756 INFO mapred.LocalJobRunner: Finishing task: attempt_local1178982250_0001_r_000000_0\n",
      "2018-11-05 17:17:43,756 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-11-05 17:17:43,889 INFO mapreduce.Job: Job job_local1178982250_0001 running in uber mode : false\n",
      "2018-11-05 17:17:43,891 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-11-05 17:17:43,892 INFO mapreduce.Job: Job job_local1178982250_0001 completed successfully\n",
      "2018-11-05 17:17:43,906 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=357342\n",
      "\t\tFILE: Number of bytes written=1377010\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=184\n",
      "\t\tMap output bytes=1288\n",
      "\t\tMap output materialized bytes=1662\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1662\n",
      "\t\tReduce input records=184\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=368\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=431489024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=507\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=520\n",
      "2018-11-05 17:17:43,907 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf output\n",
    "STREAM=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar\n",
    "chmod +x mapper.py\n",
    "chmod +x reducer.py\n",
    "hadoop jar $STREAM -input input.txt -output output  -mapper mapper.py -reducer reducer.py\n",
    "cat output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mapper.py reducer.py output input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la evaluación automática de este libro:\n",
    "\n",
    "* Abra un Terminal.\n",
    "* Asegurece que esat en la misma carpeta que contiene este notebook.\n",
    "* Salve el notebook.\n",
    "* Ejecute el siguiente comando:\n",
    "\n",
    "      ./gradetool 11-Taller.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
